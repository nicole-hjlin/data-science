{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers on Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- counts \n",
    "- networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in samples for references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. total number of tweets\n",
    "2. total number of tweets with urls \n",
    "3. total number of tweets with urls from dubious sources\n",
    "4. total number of tweets with urls from main news sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-03.json.gz\n",
      "10000 tweets\n",
      "20000 tweets\n",
      "30000 tweets\n",
      "40000 tweets\n",
      "50000 tweets\n",
      "60000 tweets\n",
      "70000 tweets\n",
      "80000 tweets\n",
      "90000 tweets\n",
      "100000 tweets\n",
      "110000 tweets\n",
      "120000 tweets\n",
      "130000 tweets\n",
      "140000 tweets\n",
      "150000 tweets\n",
      "160000 tweets\n",
      "170000 tweets\n",
      "180000 tweets\n",
      "190000 tweets\n",
      "200000 tweets\n",
      "210000 tweets\n",
      "220000 tweets\n",
      "230000 tweets\n",
      "240000 tweets\n",
      "250000 tweets\n",
      "260000 tweets\n",
      "270000 tweets\n",
      "280000 tweets\n",
      "290000 tweets\n",
      "total count: 295434\n",
      "errors: 92\n",
      "clinton-sample.json.gz\n",
      "10000 tweets\n",
      "20000 tweets\n",
      "total count: 25000\n",
      "errors: 0\n"
     ]
    }
   ],
   "source": [
    "folder = 'data/'\n",
    "fls = os.listdir(folder)\n",
    "counts = open(folder + 'sample-counts.csv', 'w')\n",
    "cwriter = csv.writer(counts)\n",
    "cwriter.writerow(['file','total count','errors','total urls','url outside of twitter','retweet','quote','reply'])\n",
    "\n",
    "for fl in fls:\n",
    "    if 'gz' in fl: \n",
    "        print(fl)\n",
    "        with gzip.open(folder+fl, \"rb\") as f:\n",
    "            total_count = 0\n",
    "            urls_count = 0\n",
    "            url_outside_twitter_count = 0\n",
    "            retweet_counts = 0\n",
    "            quote_counts = 0 \n",
    "            reply_counts = 0\n",
    "            error_count = 0\n",
    "            for line in f:\n",
    "                total_count += 1\n",
    "                if total_count % 10000 == 0:\n",
    "                    print(total_count, 'tweets')\n",
    "                try: \n",
    "                    tweet = json.loads(line.decode(\"utf-8\"))\n",
    "                    if tweet['entities']['urls'] != []: urls_count += 1\n",
    "                    if tweet['entities']['urls'] != [] and tweet['entities']['urls'][0]['expanded_url'] != None:\n",
    "                        if 'twitter.com' not in tweet['entities']['urls'][0]['expanded_url']: url_outside_twitter_count += 1\n",
    "                    if 'retweeted_status' in tweet.keys(): retweet_counts += 1\n",
    "                    if tweet['is_quote_status'] == True: quote_counts += 1\n",
    "                    if tweet['in_reply_to_status_id'] != None: reply_counts += 1\n",
    "                except: \n",
    "                    error_count += 1\n",
    "            print('total count:', total_count)\n",
    "            print('errors:', error_count)\n",
    "            row = [fl, total_count, error_count, urls_count, url_outside_twitter_count, retweet_counts, quote_counts, reply_counts]\n",
    "            cwriter.writerow(row)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be run on external hard drive as .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['hillary', 'trump']:   \n",
    "    folder = 'data/{}/'.format(name)\n",
    "    fls = os.listdir(folder)\n",
    "    counts = open(folder + '{}-counts.csv'.format(name), 'w')\n",
    "    cwriter = csv.writer(counts)\n",
    "    cwriter.writerow(['file','total count','errors','total urls','url outside of twitter','retweet','quote','reply'])\n",
    "\n",
    "    for fl in fls:\n",
    "        if 'gz' in fl: \n",
    "            print(fl)\n",
    "            with gzip.open(folder+fl, \"rb\") as f:\n",
    "                total_count = 0\n",
    "                urls_count = 0\n",
    "                url_outside_twitter_count = 0\n",
    "                retweet_counts = 0\n",
    "                quote_counts = 0 \n",
    "                reply_counts = 0\n",
    "                error_count = 0\n",
    "                for line in f:\n",
    "                    total_count += 1\n",
    "                    if total_count % 10000 == 0:\n",
    "                        print(total_count, 'tweets')\n",
    "                    try: \n",
    "                        tweet = json.loads(line.decode(\"utf-8\"))\n",
    "                        if tweet['entities']['urls'] != []: urls_count += 1\n",
    "                        if tweet['entities']['urls'] != [] and tweet['entities']['urls'][0]['expanded_url'] != None:\n",
    "                            if 'twitter.com' not in tweet['entities']['urls'][0]['expanded_url']: url_outside_twitter_count += 1\n",
    "                        if 'retweeted_status' in tweet.keys(): retweet_counts += 1\n",
    "                        if tweet['is_quote_status'] == True: quote_counts += 1\n",
    "                        if tweet['in_reply_to_status_id'] != None: reply_counts += 1\n",
    "                    except: \n",
    "                        error_count += 1\n",
    "                print('total count:', total_count)\n",
    "                print('errors:', error_count)\n",
    "                row = [fl, total_count, error_count, urls_count, url_outside_twitter_count, retweet_counts, quote_counts, reply_counts]\n",
    "                cwriter.writerow(row)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-03.json.gz\n",
      "169783 errors occured\n"
     ]
    }
   ],
   "source": [
    "folder = 'data/'\n",
    "fls = os.listdir(folder)\n",
    "\n",
    "for fl in fls:\n",
    "    if 'gz' in fl:\n",
    "        print(fl)\n",
    "        urls = open(folder + '{}-urls.csv'.format(fl[:10]), 'w')\n",
    "        uwriter = csv.writer(urls)\n",
    "        uwriter.writerow(['id','url','possibly_sensitive'])\n",
    "        with gzip.open(folder+fl, 'rb') as f:\n",
    "            error_count = 0\n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweet = json.loads(line.decode(\"utf-8\"))\n",
    "                    tweet_id = tweet['id']\n",
    "                    sen = tweet['possibly_sensitive']                 \n",
    "#                     # when the tweet is a quote and the original tweet contains url:\n",
    "#                     if tweet['is_quote_status'] == True:\n",
    "#                         if tweet['is_quote_status']['entities']['urls'] != []:\n",
    "#                             if 'unwound' in tweet['quoted_status']['entities']['urls'][0].keys():\n",
    "#                                 url = tweet['quoted_status']['unwound']['entities']['urls'][0]['unwound']['url']\n",
    "#                             if tweet['quoted_status']['entities']['urls'][0]['expanded_url'] != '':\n",
    "#                                 url = tweet['quoted_status']['entities']['urls'][0]['expanded_url']\n",
    "#                             else: \n",
    "#                                 url = tweet['quoted_status']['entities']['urls'][0]['url']\n",
    "#                     # when the tweet is a retweet and the original tweet contains url:\n",
    "#                     if 'retweeted_status' in tweet.keys():\n",
    "#                         if tweet['retweeted_status']['entities']['urls'] != []:\n",
    "#                             if 'unwound' in tweet['retweeted_status']['entities']['urls'][0].keys():\n",
    "#                                 url = tweet['retweeted_status']['unwound']['entities']['urls'][0]['unwound']['url']\n",
    "#                             if tweet['retweeted_status']['entities']['urls'][0]['expanded_url'] != '':\n",
    "#                                 url = tweet['retweeted_status']['entities']['urls'][0]['expanded_url']\n",
    "#                             else: \n",
    "#                                 url = tweet['retweeted_status']['entities']['urls'][0]['url']\n",
    "#                     # if the tweet is original and it contains url:\n",
    "#                     else:\n",
    "                    if tweet['entities']['urls'] != []:\n",
    "                        if 'unwound' in tweet['entities']['urls'][0].keys():\n",
    "                            url = tweet['unwound']['entities']['urls'][0]['unwound']['url']\n",
    "                        if tweet['entities']['urls'][0]['expanded_url'] != '':\n",
    "                            url = tweet['entities']['urls'][0]['expanded_url']\n",
    "                        else: \n",
    "                            url = tweet['entities']['urls'][0]['url']             \n",
    "                    row = [tweet_id, url, sen]\n",
    "                    uwriter.writerow(row)\n",
    "                except: error_count +=1\n",
    "            print('finished with',error_count, 'errors occured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
